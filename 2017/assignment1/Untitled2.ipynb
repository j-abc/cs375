{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tabular as tb\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from dldata.metrics.utils import compute_metric_base\n",
    "from tfutils import base, data, model, optimizer, utils\n",
    "\n",
    "from utils import post_process_neural_regression_msplit_preprocessed\n",
    "from dataprovider import NeuralDataProvider\n",
    "from models import alexnet_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Welcome to the second part of the assignment 1! In this section, we will learn\n",
    "how to analyze our trained model and evaluate its performance on predicting\n",
    "neural data.\n",
    "Mainly, you will first learn how to load your trained model from the database\n",
    "and then how to use tfutils to evaluate your model on neural data using dldata.\n",
    "The evaluation will be performed using the 'agg_func' in 'validation_params',\n",
    "which operates on the aggregated validation results obtained from running the\n",
    "model on the stimulus images. So let's get started!\n",
    "\n",
    "Note: Although you will only have to edit a small fraction of the code at the\n",
    "beginning of the assignment by filling in the blank spaces, you will need to \n",
    "build on the completed starter code to fully complete the assignment,\n",
    "We expect that you familiarize yourself with the codebase and learn how to\n",
    "setup your own experiments taking the assignments as a basis. This code does\n",
    "not cover all parts of the assignment and only provides a starting point. To\n",
    "fully complete the assignment significant changes have to be made and new \n",
    "functions need to be added after filling in the blanks. Also, for your projects\n",
    "we won't give out any code and you will have to use what you have learned from\n",
    "your assignments. So please always carefully read through the entire code and\n",
    "try to understand it. If you have any questions about the code structure,\n",
    "we will be happy to answer it.\n",
    "\n",
    "Attention: All sections that need to be changed to complete the starter code\n",
    "are marked with EDIT!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tabular as tb\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from dldata.metrics.utils import compute_metric_base\n",
    "from tfutils import base, data, model, optimizer, utils\n",
    "\n",
    "from utils import post_process_neural_regression_msplit_preprocessed\n",
    "from dataprovider import NeuralDataProvider\n",
    "from models import alexnet_model\n",
    "\n",
    "\n",
    "class NeuralDataExperiment():\n",
    "    \"\"\"\n",
    "    Defines the neural data testing experiment\n",
    "    \"\"\"\n",
    "    class Config():\n",
    "        \"\"\"\n",
    "        Holds model hyperparams and data information.\n",
    "        The config class is used to store various hyperparameters and dataset\n",
    "        information parameters. You will need to change the target layers,\n",
    "        exp_id, and might have to modify 'conv1_kernel' to the name of your\n",
    "        first layer, once you start working with different models. Set the seed \n",
    "        number to your group number. But please do not change the rest. \n",
    "                         'pool2', \n",
    "                         'conv3', \n",
    "                         'conv4', \n",
    "                         'conv5', \n",
    "                         'pool5', \n",
    "                         'fc6', \n",
    "                         'fc7',\n",
    "                         'fc8',\n",
    "                         'conv1',\n",
    "                         'conv2']\n",
    "        You will have to EDIT this part. Please set your exp_id here.\n",
    "        \"\"\"\n",
    "        \n",
    "        target_layers = ['pool1'\n",
    "                         ]\n",
    "        extraction_step = None\n",
    "        exp_id = 'experiment_1'\n",
    "        data_path = '/datasets/neural_data/tfrecords_with_meta'\n",
    "        noise_estimates_path = '/datasets/neural_data/noise_estimates.npy'\n",
    "        batch_size = 128\n",
    "        seed = 6\n",
    "        crop_size = 227\n",
    "        gfs_targets = [] \n",
    "        extraction_targets = [attr[0] for attr in NeuralDataProvider.ATTRIBUTES] \\\n",
    "            + target_layers + ['conv1_kernel']\n",
    "        assert NeuralDataProvider.N_VAL % batch_size == 0, \\\n",
    "                ('number of examples not divisible by batch size!')\n",
    "        val_steps = int(NeuralDataProvider.N_VAL / batch_size)\n",
    "\n",
    "\n",
    "    def setup_params(self):\n",
    "        \"\"\"\n",
    "        This function illustrates how to setup up the parameters for train_from_params\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "\n",
    "        \"\"\"\n",
    "        validation_params similar to train_params defines the validation parameters.\n",
    "        It has the same arguments as train_params and additionally\n",
    "            agg_func: function that aggregates the validation results across batches,\n",
    "                e.g. to calculate the mean of across batch losses\n",
    "            online_agg_func: function that aggregates the validation results across\n",
    "                batches in an online manner, e.g. to calculate the RUNNING mean across\n",
    "                batch losses\n",
    "\n",
    "        Note: Note how we switched the data provider from the ImageNetDataProvider \n",
    "        to the NeuralDataProvider since we are now working with the neural data.\n",
    "        \"\"\"\n",
    "        params['validation_params'] = {\n",
    "            'valid0': {\n",
    "                'data_params': {\n",
    "                    # ImageNet data provider arguments\n",
    "                    'func': NeuralDataProvider,\n",
    "                    'data_path': self.Config.data_path,\n",
    "                    'crop_size': self.Config.crop_size,\n",
    "                    # TFRecords (super class) data provider arguments\n",
    "                    'file_pattern': '*.tfrecords',\n",
    "                    'batch_size': self.Config.batch_size,\n",
    "                    'shuffle': False,\n",
    "                    'shuffle_seed': self.Config.seed, \n",
    "                    'n_threads': 1,\n",
    "                },\n",
    "                'queue_params': {\n",
    "                    'queue_type': 'fifo',\n",
    "                    'batch_size': self.Config.batch_size,\n",
    "                    'seed': self.Config.seed,\n",
    "                    'capacity': self.Config.batch_size * 10,\n",
    "                    'min_after_dequeue': self.Config.batch_size * 1,\n",
    "                },\n",
    "                'targets': {\n",
    "                    'func': self.return_outputs,\n",
    "                    'targets': self.Config.extraction_targets,\n",
    "                },\n",
    "                'num_steps': self.Config.val_steps, #CHANGED HERE\n",
    "                #'num_steps': 1,\n",
    "                'agg_func': self.neural_analysis,\n",
    "                'online_agg_func': self.online_agg,\n",
    "            }\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "        model_params defines the model i.e. the architecture that \n",
    "        takes the output of the data provider as input and outputs \n",
    "        the prediction of the model.\n",
    "\n",
    "        You will need to EDIT this part. Switch out the model 'func' as \n",
    "        needed when running experiments on different models. The default\n",
    "        is set to the alexnet model you implemented in the first part of the\n",
    "        assignment.\n",
    "        \"\"\"\n",
    "        params['model_params'] = {\n",
    "            'func': alexnet_model,\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "        save_params defines how, where and when your training results are saved\n",
    "        in the database.\n",
    "\n",
    "        You will need to EDIT this part. Set your own 'host' ('localhost' if local,\n",
    "        mongodb IP if remote mongodb), 'port', 'dbname', and 'collname' if you want\n",
    "        to evaluate on a different model than the pretrained alexnet model.\n",
    "        'exp_id' has to be set in Config.\n",
    "        \"\"\"\n",
    "        params['save_params'] = {\n",
    "            'host': 'localhost',\n",
    "            'port': 24444,\n",
    "            'dbname': 'imagenet',\n",
    "            'collname': 'alexnet',\n",
    "            'exp_id': self.Config.exp_id,\n",
    "            'save_to_gfs': self.Config.gfs_targets,\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "        load_params defines how and if a model should be restored from the database.\n",
    "\n",
    "        You will need to EDIT this part. Set your own 'host' ('localhost' if local,\n",
    "        mongodb IP if remote mongodb), 'port', 'dbname', and 'collname' if you want\n",
    "        to evaluate on a different model than the pretrained alexnet model.\n",
    "        'exp_id' has to be set in Config.\n",
    "        \"\"\"\n",
    "        params['load_params'] = {\n",
    "            'host': 'localhost',\n",
    "            'port': 24444,\n",
    "            'dbname': 'imagenet',\n",
    "            'collname': 'alexnet',\n",
    "            'exp_id': self.Config.exp_id,\n",
    "            'do_restore': True,\n",
    "            'query': {'step': self.Config.extraction_step} \\\n",
    "                    if self.Config.extraction_step is not None else None,\n",
    "        }\n",
    "\n",
    "        params['inter_op_parallelism_threads'] = 500\n",
    "\n",
    "        return params\n",
    "\n",
    "\n",
    "    def return_outputs(self, inputs, outputs, targets, **kwargs):\n",
    "        \"\"\"\n",
    "        Illustrates how to extract desired targets from the model\n",
    "        \"\"\"\n",
    "        retval = {}\n",
    "        for target in targets:\n",
    "            retval[target] = outputs[target]\n",
    "        return retval\n",
    "\n",
    "\n",
    "    def online_agg(self, agg_res, res, step):\n",
    "        \"\"\"\n",
    "        Appends the value for each key\n",
    "        \"\"\"\n",
    "        if agg_res is None:\n",
    "            agg_res = {k: [] for k in res}\n",
    "        for k, v in res.items():\n",
    "            if 'kernel' in k:\n",
    "                agg_res[k] = v\n",
    "            else:\n",
    "                agg_res[k].append(v)\n",
    "        return agg_res\n",
    "\n",
    "\n",
    "    def parse_meta_data(self, results):\n",
    "        \"\"\"\n",
    "        Parses the meta data from tfrecords into a tabarray\n",
    "        \"\"\"\n",
    "        meta_keys = [attr[0] for attr in NeuralDataProvider.ATTRIBUTES \\\n",
    "                if attr[0] not in ['images', 'it_feats']]\n",
    "        meta = {}\n",
    "        for k in meta_keys:\n",
    "            if k not in results:\n",
    "                raise KeyError('Attribute %s not loaded' % k)\n",
    "            meta[k] = np.concatenate(results[k], axis=0)\n",
    "        return tb.tabarray(columns=[list(meta[k]) for k in meta_keys], names = meta_keys)\n",
    "\n",
    "\n",
    "    def categorization_test(self, features, meta):\n",
    "        \"\"\"\n",
    "        Performs a categorization test using dldata\n",
    "\n",
    "        You will need to EDIT this part. Define the specification to\n",
    "        do a categorization on the neural stimuli using \n",
    "        compute_metric_base from dldata.\n",
    "        \"\"\"\n",
    "        print('Categorization test...')\n",
    "        category_eval_spec = {\n",
    "            'npc_train': None,\n",
    "            'npc_test': 2,\n",
    "            'num_splits': 20,\n",
    "            'npc_validate': 0,\n",
    "            'metric_screen': 'classifier',\n",
    "            'metric_labels': None,\n",
    "            'metric_kwargs': {'model_type': 'svm.LinearSVC',\n",
    "                              'model_kwargs': {'C':5e-3}\n",
    "                             },\n",
    "            'labelfunc': 'category',\n",
    "            'train_q': {'var': ['V0', 'V3', 'V6']},\n",
    "            'test_q': {'var': ['V0', 'V3', 'V6']},\n",
    "            'split_by': 'obj'\n",
    "        }\n",
    "        res = compute_metric_base(features, meta, category_eval_spec)\n",
    "        res.pop('split_results')\n",
    "        return res\n",
    "    \n",
    "    def within_categorization_test(self, features, meta):\n",
    "        \"\"\"\n",
    "        Performs a categorization test using dldata\n",
    "\n",
    "        You will need to EDIT this part. Define the specification to\n",
    "        do a categorization on the neural stimuli using \n",
    "        compute_metric_base from dldata.\n",
    "        \"\"\"\n",
    "        # convert res to a dictionary for each category\n",
    "        res = {}\n",
    "        category_set = np.unique(meta['category'])\n",
    "        print(category_set)\n",
    "        \"\"\"\n",
    "        for ic in category_set:\n",
    "            print('Within categorization test for %s...'%ic)\n",
    "            \n",
    "            category_eval_spec = {\n",
    "                'npc_train': None,\n",
    "                'npc_test': 2,\n",
    "                'num_splits': 20,\n",
    "                'npc_validate': 0,\n",
    "                'metric_screen': 'classifier',\n",
    "                'metric_labels': None,\n",
    "                'metric_kwargs': {'model_type': 'svm.LinearSVC',\n",
    "                                  'model_kwargs': {'C':5e-3}\n",
    "                                 },\n",
    "                'labelfunc': 'obj',\n",
    "                'train_q': {'var': ['V0', 'V3', 'V6'], 'category': [ic]},\n",
    "                'test_q': {'var': ['V0', 'V3', 'V6'], 'category': [ic]},\n",
    "                'split_by': 'obj'\n",
    "            }\n",
    "            res[ic] = compute_metric_base(features, meta, category_eval_spec)\n",
    "            res[ic].pop('split_results')   \n",
    "            \"\"\"\n",
    "        return res\n",
    "\n",
    "    def regression_test(self, features, IT_features, meta):\n",
    "        \"\"\"\n",
    "        Illustrates how to perform a regression test using dldata\n",
    "\n",
    "        You will need to EDIT this part. Define the specification to\n",
    "        do a regression on the IT neurons using compute_metric_base from dldata.\n",
    "        \"\"\"\n",
    "        print('Regression test...')\n",
    "        it_reg_eval_spec = {\n",
    "            'npc_train': 70,\n",
    "            'npc_test': 10,\n",
    "            'num_splits': 5,\n",
    "            'npc_validate': 0,\n",
    "            'metric_screen': 'regression',\n",
    "            'metric_labels': None,\n",
    "            'metric_kwargs': {\n",
    "                'model_type': 'linear_model.RidgeCV',\n",
    "                             },\n",
    "            'labelfunc': lambda x: (IT_features, None),\n",
    "            'train_q': {'var': ['V0', 'V3', 'V6']},\n",
    "            'test_q': {'var': ['V0', 'V3', 'V6']},\n",
    "            'split_by': 'obj'\n",
    "        }\n",
    "        res = compute_metric_base(features, meta, it_reg_eval_spec)\n",
    "        espec = (('all','','IT_regression'), it_reg_eval_spec)\n",
    "        post_process_neural_regression_msplit_preprocessed(\n",
    "                res, self.Config.noise_estimates_path)\n",
    "        res.pop('split_results')\n",
    "        return res\n",
    "\n",
    "\n",
    "    def compute_rdm(self, features, meta, mean_objects=False):\n",
    "        \"\"\"\n",
    "        Computes the RDM of the input features\n",
    "\n",
    "        You will need to EDIT this part. Compute the RDM of features which is a\n",
    "        [N_IMAGES x N_FEATURES] matrix. The features are then averaged across\n",
    "        images of the same category which creates a [N_CATEGORIES x N_FEATURES]\n",
    "        matrix that you have to work with.\n",
    "        \"\"\"\n",
    "        print('Computing RDM...')\n",
    "        if mean_objects:\n",
    "            object_list = list(itertools.chain(\n",
    "                *[np.unique(meta[meta['category'] == c]['obj']) \\\n",
    "                        for c in np.unique(meta['category'])]))\n",
    "            features = np.array([features[(meta['obj'] == o.rstrip('_'))].mean(0) \\\n",
    "                    for o in object_list])\n",
    "        ### YOUR CODE HERE\n",
    "        rdm = 1 - np.corrcoef(features)\n",
    "        ### END OF YOUR CODE\n",
    "        return rdm\n",
    "\n",
    "\n",
    "    def get_features(self, results, num_subsampled_features=None):\n",
    "        \"\"\"\n",
    "        Extracts, preprocesses and subsamples the target features\n",
    "        and the IT features\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        for layer in self.Config.target_layers:\n",
    "            feats = np.concatenate(results[layer], axis=0)\n",
    "            feats = np.reshape(feats, [feats.shape[0], -1])\n",
    "            if num_subsampled_features is not None:\n",
    "                features[layer] = \\\n",
    "                        feats[:, np.random.RandomState(0).permutation(\n",
    "                            feats.shape[1])[:num_subsampled_features]]\n",
    "\n",
    "        IT_feats = np.concatenate(results['it_feats'], axis=0)\n",
    "\n",
    "        return features, IT_feats\n",
    "\n",
    "\n",
    "    def neural_analysis(self, results):\n",
    "        \"\"\"\n",
    "        Performs an analysis of the results from the model on the neural data.\n",
    "        This analysis includes:\n",
    "            - saving the conv1 kernels\n",
    "            - computing a RDM\n",
    "            - a categorization test\n",
    "            - and an IT regression.\n",
    "\n",
    "        You will need to EDIT this function to fully complete the assignment.\n",
    "        Add the necessary analyses as specified in the assignment pdf.\n",
    "        \"\"\"\n",
    "        retval = {'conv1_kernel': results['conv1_kernel']}\n",
    "        print('Performing neural analysis...')\n",
    "        meta = self.parse_meta_data(results)\n",
    "        features, IT_feats = self.get_features(results, num_subsampled_features=1024)\n",
    "\n",
    "        print('IT:')\n",
    "        \"\"\"\n",
    "        retval['rdm_it'] = \\\n",
    "                self.compute_rdm(IT_feats, meta, mean_objects=True)\n",
    "        \"\"\"\n",
    "        for layer in features:\n",
    "            \n",
    "            print('Layer: %s' % layer)\n",
    "            \"\"\"\n",
    "            # RDM\n",
    "            retval['rdm_%s' % layer] = \\\n",
    "                    self.compute_rdm(features[layer], meta, mean_objects=True)\n",
    "            # RDM correlation\n",
    "            retval['spearman_corrcoef_%s' % layer] = \\\n",
    "                    spearmanr(\n",
    "                            np.reshape(retval['rdm_%s' % layer], [-1]),\n",
    "                            np.reshape(retval['rdm_it'], [-1])\n",
    "                            )[0]\n",
    "            # categorization test\n",
    "            retval['categorization_%s' % layer] = \\\n",
    "                    self.categorization_test(features[layer], meta)\n",
    "            \n",
    "            # IT regression test\n",
    "            retval['it_regression_%s' % layer] = \\\n",
    "                    self.regression_test(features[layer], IT_feats, meta)\n",
    "            \"\"\"\n",
    "            retval['within_categorization_%s' % layer] = \\\n",
    "                    self.within_categorization_test(features[layer], meta)\n",
    "        return retval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [-p PARAMS] [-g GPU]\n",
      "__main__.py: error: unrecognized arguments: -f /run/user/1002/jupyter/kernel-ae9990b3-975e-415c-b98b-77a5565b685b.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "base.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = NeuralDataExperiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-980bffe1404d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiment_1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.Config.exp_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obj',\n",
       " 'rxz',\n",
       " 'rxy',\n",
       " 'ryz',\n",
       " 'ty',\n",
       " 'tz',\n",
       " 's',\n",
       " 'bg_id',\n",
       " 'size',\n",
       " 'var',\n",
       " '_id',\n",
       " 'filename',\n",
       " 'id',\n",
       " 'category',\n",
       " 'rxz_semantic',\n",
       " 'rxy_semantic',\n",
       " 'ryz_semantic',\n",
       " 'centroid_x',\n",
       " 'centroid_y',\n",
       " 'volume',\n",
       " 'perimeter',\n",
       " 'axis_bb_left',\n",
       " 'axis_bb_top',\n",
       " 'axis_bb_right',\n",
       " 'axis_bb_bottom',\n",
       " 'axis_bb_ctr_x',\n",
       " 'axis_bb_ctr_y',\n",
       " 'axis_bb_xlen',\n",
       " 'axis_bb_ylen',\n",
       " 'axis_bb_area',\n",
       " 'axis_bb_ratio',\n",
       " 'area_bb_0_x',\n",
       " 'area_bb_0_y',\n",
       " 'area_bb_1_x',\n",
       " 'area_bb_1_y',\n",
       " 'area_bb_2_x',\n",
       " 'area_bb_2_y',\n",
       " 'area_bb_3_x',\n",
       " 'area_bb_3_y',\n",
       " 'area_bb_minlen',\n",
       " 'area_bb_maxlen',\n",
       " 'area_bb_ratio',\n",
       " 'area_bb_major0_x',\n",
       " 'area_bb_major0_y',\n",
       " 'area_bb_major1_x',\n",
       " 'area_bb_major1_y',\n",
       " 'area_bb_sine_major',\n",
       " 'area_bb_cos_major',\n",
       " 'images',\n",
       " 'it_feats',\n",
       " 'pool1',\n",
       " 'conv1_kernel']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.Config.extraction_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base.train_from_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dldata.metrics.utils.compute_metric_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
