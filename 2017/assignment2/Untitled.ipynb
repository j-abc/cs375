{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import test_models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class model_switcher:\n",
    "    '''\n",
    "    Description:\n",
    "        model switcher is a wrapper around models.py.\n",
    "    Inputs:\n",
    "        model_name: name of model within models.py\n",
    "        data_name:  name of data source\n",
    "    Stores:\n",
    "        dbname:   as data_name\n",
    "        collname: as model_name\n",
    "        layers:   as layer names associated with our model\n",
    "                  for now this is hard coded\n",
    "        model_fn: a reference to the model definition as imported from models.py\n",
    "    '''\n",
    "    def __init__(self, model_name = 'herpaderp', data_name = 'cifar10'):\n",
    "        # actual variables\n",
    "        self.data_name  = data_name\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # variables that we feed into train and test.py\n",
    "        self.dbname     = data_name\n",
    "        self.collname   = model_name\n",
    "        self.layers     = self._model_layers(model_name)\n",
    "        self.model_fn   = self._get_model_fn(model_name)\n",
    "        \n",
    "    def _get_model_fn(self, model_name):\n",
    "        if hasattr(models, model_name):\n",
    "            return getattr(models,model_name)\n",
    "        else:\n",
    "            raise Exception('Model name not found in models.py')\n",
    "            \n",
    "    def _model_layers(self, model_name):\n",
    "        layer_dict = {\n",
    "            'herpaderp':['test', 'test','test'],\n",
    "            'tiny_model': ['blah']\n",
    "        }\n",
    "\n",
    "        if model_name not in layer_dict.keys():\n",
    "            raise Exception('Model layer names not specified')\n",
    "        else:\n",
    "            return layer_dict[self.model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Model name not found in models.py",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-6bd773a6ba8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_switcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'herpaderp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-47-6a75cf287096>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_name, data_name)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollname\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_fn\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-6a75cf287096>\u001b[0m in \u001b[0;36m_get_model_fn\u001b[1;34m(self, model_name)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model name not found in models.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_model_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Model name not found in models.py"
     ]
    }
   ],
   "source": [
    "test = model_switcher('herpaderp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function test_models.tiny_model>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "def tiny_model(inputs, train = True, norm = True, **kwargs):\n",
    "\n",
    "    # propagate input targets\n",
    "    outputs = inputs\n",
    "    dropout = .5 if train else None\n",
    "    input_to_network = inputs['images']\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # set up all layer outputs\n",
    "    # set up all layer outputs\n",
    "    outputs['conv1'],outputs['conv1_kernel']  = conv(outputs['images'], 96, 11, 4, padding='VALID', layer = 'conv1')\n",
    "    lrn1 = outputs['conv1']\n",
    "    if norm:\n",
    "        lrn1 = lrn(outputs['conv1'], depth_radius=5, bias=1, alpha=.0001, beta=.75, layer='conv1')\n",
    "    outputs['pool1'] = max_pool(lrn1, 3, 2, layer = 'pool1')\n",
    "    outputs['conv2'], outputs['conv2_kernel'] = conv(outputs['pool1'], 256, 5, 1, layer = 'conv2')\n",
    "    lrn2 = outputs['conv2']\n",
    "    if norm:\n",
    "        lrn2 = lrn(outputs['conv2'], depth_radius=5, bias=1, alpha=.0001, beta=.75, layer='conv2')\n",
    "        \n",
    "    outputs['pool2'] = max_pool(lrn2, 3, 2, layer = 'pool2')\n",
    "    outputs['fc6'] = fc(outputs['pool2'], 4096, dropout=dropout, bias=.1, layer = 'fc6')\n",
    "    outputs['fc7'] = fc(outputs['fc6'], 4096, dropout=dropout, bias=.1, layer = 'fc7')\n",
    "    outputs['fc8'] = fc(outputs['fc7'],1000, activation=None, dropout=None, bias=0, layer = 'fc8')\n",
    "\n",
    "    outputs['pred'] = outputs['fc8']\n",
    "    return outputs, {}     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(inp,\n",
    "         out_depth,\n",
    "         ksize=[3,3],\n",
    "         strides=[1,1,1,1],\n",
    "         padding='SAME',\n",
    "         kernel_init='xavier',\n",
    "         kernel_init_kwargs=None,\n",
    "         bias=0,\n",
    "         weight_decay=None,\n",
    "         activation='relu',\n",
    "         batch_norm=False,\n",
    "         name='conv',\n",
    "         layer = None,\n",
    "         ):\n",
    "    with tf.variable_scope(layer):\n",
    "        # assert out_shape is not None\n",
    "        if weight_decay is None:\n",
    "            weight_decay = 0.\n",
    "        if isinstance(ksize, int):\n",
    "            ksize = [ksize, ksize]\n",
    "            \n",
    "        if isinstance(strides, int):\n",
    "            strides = [1, strides, strides, 1]            \n",
    "            \n",
    "        if kernel_init_kwargs is None:\n",
    "            kernel_init_kwargs = {}\n",
    "        in_depth = inp.get_shape().as_list()[-1]\n",
    "\n",
    "        # weights\n",
    "        init = initializer(kernel_init, **kernel_init_kwargs)\n",
    "\n",
    "\n",
    "        kernel = tf.get_variable(initializer=init,\n",
    "                                shape=[ksize[0], ksize[1], in_depth, out_depth],\n",
    "                                dtype=tf.float32,\n",
    "                                regularizer=tf.contrib.layers.l2_regularizer(weight_decay),\n",
    "                                name='weights')\n",
    "        init = initializer(kind='constant', value=bias)\n",
    "        biases = tf.get_variable(initializer=init,\n",
    "                                shape=[out_depth],\n",
    "                                dtype=tf.float32,\n",
    "                                regularizer=tf.contrib.layers.l2_regularizer(weight_decay),\n",
    "                                name='bias')\n",
    "        # ops\n",
    "        conv = tf.nn.conv2d(inp, kernel,\n",
    "                            strides=strides,\n",
    "                            padding=padding)\n",
    "        output = tf.nn.bias_add(conv, biases, name=name)\n",
    "\n",
    "        if activation is not None:\n",
    "            output = getattr(tf.nn, activation)(output, name=activation)\n",
    "        if batch_norm:\n",
    "            output = tf.nn.batch_normalization(output, mean=0, variance=1, offset=None,\n",
    "                                scale=None, variance_epsilon=1e-8, name='batch_norm')\n",
    "    return output, kernel\n",
    "\n",
    "def max_pool(x, ksize, strides,  name='pool', padding='SAME', layer = None):\n",
    "    with tf.variable_scope(layer):\n",
    "        if isinstance(ksize, int):\n",
    "            ksize = [ksize, ksize]\n",
    "        if isinstance(strides, int):\n",
    "            strides = [1, strides, strides, 1]\n",
    "    return tf.nn.max_pool(x, ksize= [1, ksize[0], ksize[1],1],\n",
    "                        strides = strides,\n",
    "                        padding = padding, name = name)\n",
    "\n",
    "def fc(inp,\n",
    "       out_depth,\n",
    "       kernel_init='xavier',\n",
    "       kernel_init_kwargs=None,\n",
    "       bias=1,\n",
    "       weight_decay=None,\n",
    "       activation='relu',\n",
    "       batch_norm=True,\n",
    "       dropout=None,\n",
    "       dropout_seed=None,\n",
    "       name='fc',\n",
    "       layer='blah'):\n",
    "    with tf.variable_scope(layer):\n",
    "        if weight_decay is None:\n",
    "            weight_decay = 0.\n",
    "        # assert out_shape is not None\n",
    "        if kernel_init_kwargs is None:\n",
    "            kernel_init_kwargs = {}\n",
    "        resh = tf.reshape(inp, [inp.get_shape().as_list()[0], -1], name='reshape')\n",
    "        in_depth = resh.get_shape().as_list()[-1]\n",
    "\n",
    "        # weights\n",
    "        init = initializer(kernel_init, **kernel_init_kwargs)\n",
    "        kernel = tf.get_variable(initializer=init,\n",
    "                                shape=[in_depth, out_depth],\n",
    "                                dtype=tf.float32,\n",
    "                                regularizer=tf.contrib.layers.l2_regularizer(weight_decay),\n",
    "                                name='weights')\n",
    "        init = initializer(kind='constant', value=bias)\n",
    "        biases = tf.get_variable(initializer=init,\n",
    "                                shape=[out_depth],\n",
    "                                dtype=tf.float32,\n",
    "                                regularizer=tf.contrib.layers.l2_regularizer(weight_decay),\n",
    "                                name='bias')\n",
    "\n",
    "        # ops\n",
    "        fcm = tf.matmul(resh, kernel)\n",
    "        output = tf.nn.bias_add(fcm, biases, name=name)\n",
    "\n",
    "        if activation is not None:\n",
    "            output = getattr(tf.nn, activation)(output, name=activation)\n",
    "        if batch_norm:\n",
    "            output = tf.nn.batch_normalization(output, mean=0, variance=1, offset=None,\n",
    "                                scale=None, variance_epsilon=1e-8, name='batch_norm')\n",
    "        if dropout is not None:\n",
    "            output = tf.nn.dropout(output, dropout, seed=dropout_seed, name='dropout')\n",
    "    return output\n",
    "\n",
    "def initializer(kind='xavier', *args, **kwargs):\n",
    "    if kind == 'xavier':\n",
    "        init = tf.contrib.layers.xavier_initializer(*args, **kwargs)\n",
    "    else:\n",
    "        init = getattr(tf, kind + '_initializer')(*args, **kwargs)\n",
    "    return init\n",
    "\n",
    "\n",
    "def lrn(inp,\n",
    "    depth_radius=5, \n",
    "    bias=1, \n",
    "    alpha=.0001, \n",
    "    beta=.75, \n",
    "    name = 'lrn',\n",
    "    layer = None):\n",
    "    with tf.variable_scope(layer):\n",
    "        lrn = tf.nn.local_response_normalization(inp, depth_radius = depth_radius, alpha = alpha,\n",
    "                                            beta = beta, bias = bias, name = name)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
